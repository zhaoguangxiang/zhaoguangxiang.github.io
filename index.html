
<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>Zhao Guangxiang(Peking University)</title>
<style type="text/css">
.name {font-size: 24px;}
body {background-color: #FFF;}
body,td,th {font-size: 22px;}
.size20 {font-size: 22px;
}
.new_tag {
	font-family: "Arial Black", Gadget, sans-serif;
	color: #F00;
}
</style>
<style type="text/css" rel="stylesheet">
a{
color:color:blue;text-decoration:none;
}
a:hover{
color:#A52A2A;text-decoration:none;
}
a:active{
color:blue;text-decoration:none;
}
</style>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131156519-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	    gtag('config', 'UA-131156519-1');
		</script>

<script>
function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
  }
  dest.blur();
}
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>

<table cellspacing="0" cellpadding="0" border="0" width="1200">
<tr>
<td width="16%" height="120"><img src="https://tobiaslee-1252728629.cos.ap-chengdu.myqcloud.com/ll.png" width="180" /></td>

<td width="84%">
<body>
 <p><strong>Guangxiang Zhao (赵光香）</strong></p>
 <p>PHD Student, School of EECS, <a href="https://www.pku.edu.cn/">Peking University</a></p>
 <p>I am doing research at <br> <a href="http://lanco.pku.edu.cn/">Language Computing and Machine Learning Group (LANCO)</a>, <br> advised by Prof. <a href="https://xusun.org/">Xu SUN</a>.</p>
<!-- <p><a href="https://tobiaslee.top">Blog</a> <br> -->
  <a href="https://github.com/zhaoguangxiang">GitHub</a> <br>
  <a href="https://twitter.com/GuangxiangZ">Twitter</a> <br>
  <a href='https://scholar.google.com/citations?user=0IXhrDMAAAAJ&hl=zh-CN'>Google Scholar</a><br>
 <a href="https://openreview.net/profile?id=~guangxiang_zhao2&mode=view">OpenReview</a> <br>
  <p>Email: zhaoguangxiang at pku.edu.cn / guangxiangzhao at gmail.com</p>
</td>
 
</tr>
</table>
 
<p style="font-size: 12px; height: 5px">&nbsp;</p>
 <h3>Research Interests:</h3>
<p> Deep Learning for Natural Language Processing</p>

<h3>Papers: </h3>
<ul>

	<li>
Parallel Intersected Multi-scale Attention for Sequence to Sequence Learning <br>
<b>Guangxiang Zhao</b>, Xu Sun, Jingjing Xu,  Zhiyuan Zhang, Liangcheng Luo. <br>
Preprint 2019 <br>
TLDR: A simple module consistently outperforms self-attention and Transformer model on main NMT datasets with SoTA performance; find that shared projection is essential for combining the convolution and self attention <br>

<a href=https://arxiv.org/abs/1911.09483>[pdf]</a> <a href=https://github.com/lancopku/Prime>[code, scripts, and pretrained models] </a>
<br><br>


			<li>
Explicit Sparse Transformer <br>
<b>Guangxiang Zhao</b>, Junyang Lin, Qi Zeng, Xuancheng Ren, Qi Su, Xu Sun. <br>
Preprint 2019 <br>
TLDR: Propose to sparse attention weights in transformer according to their activations; give evidence that sparse attention(8 or 1/4 of the sequence length(30) in NMT) is better; sparse attention without local attention constraint. <br>

<a href=https://arxiv.org/pdf/1912.11637>[pdf]</a> <a href=https://github.com/lancopku/Explicit-Sparse-Transformer>[code] </a>
<br><br>
				
	
<li>
Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations <br>
<b>Guangxiang Zhao*</b>, Jingjing Xu* (Equal Contribution), Qi Zeng, Xuancheng Ren, Xu Sun.<br>
NAACL 2019 <br>
TLDR: Build a multi-label text classification dataset with strong label correlations, propose a method that automatically learn correlation during training. <br>

<a href=https://www.aclweb.org/anthology/N19-1296.pdf>[pdf]</a> <a href=https://github.com/lancopku/RMSC>[data] </a>
<br><br>

	<li>
Understanding and Improving Layer Normalization <br>
Jingjing Xu, Xu Sun, Zhiyuan Zhang,<b> Guangxiang Zhao</b>, Junyang Lin. <br>
NeurIPS 2019 <br>
TLDR: Find that the backprop of layernorm is essential and propose better normalization method. <br>
<a href=https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization>[pdf]</a> <a href=https://github.com/lancopku/AdaNorm>[code] </a>
<br><br>
	<li>
Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning <br>
Fenglin Liu, Xuancheng Ren, <b>Guangxiang Zhao</b>, Xu Sun <br>
Preprint 2020 <br>
TLDR: Find a limitation about information flow in Transformer  and propose an effective cross-view decoding method to solve it. <br>
<a href=https://arxiv.org/pdf/2005.08081>[pdf]</a>
<br><br>		
		
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>

<h3>Awards:</h3>
<ul>
<li>
Merit Student of Peking University, 2019 <br>
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>

<h3>Service:</h3>
<ul>
<li>
As a subreviewer at ACL 2019, EMNLP 2020 <br>
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>
</body>
</html>
