
<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>Zhao Guangxiang(Peking University)</title>
<style type="text/css">
.name {font-size: 24px;}
body {background-color: #FFF;}
body,td,th {font-size: 22px;}
.size20 {font-size: 22px;
}
.new_tag {
	font-family: "Arial Black", Gadget, sans-serif;
	color: #F00;
}
</style>
<style type="text/css" rel="stylesheet">
a{
color:color:blue;text-decoration:none;
}
a:hover{
color:#A52A2A;text-decoration:none;
}
a:active{
color:blue;text-decoration:none;
}
</style>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131156519-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	    gtag('config', 'UA-131156519-1');
		</script>

<script>
function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
  }
  dest.blur();
}
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</head>

<table cellspacing="0" cellpadding="0" border="0" width="1200">
<tr>
<td width="16%" height="120"><img src="6.jpg" width="180" /></td>

<td width="84%">
<body>
 <p><strong>Guangxiang Zhao (赵光香）</strong></p>
 <p>Researcher at <a href="Pujiang Lab">Pujiang Lab</a> since August 2022. </p>
 <p> Before joing here, I received Doctor of Science from <a href="https://www.pku.edu.cn/">Peking University</a>, <br> and  did research at <a href="https://lancopku.github.io/">Language Computing and Machine Learning Group (LANCO)</a>, <br> advised by Prof. <a href="https://xusun.org/">Xu SUN</a>.</p>
<!-- <p><a href="https://tobiaslee.top">Blog</a> <br> -->
  <a href="https://github.com/zhaoguangxiang">GitHub</a> <br>
  <a href="https://twitter.com/GuangxiangZ">Twitter</a> <br>
  <a href='https://scholar.google.com/citations?user=0IXhrDMAAAAJ&hl=zh-CN'>Google Scholar</a><br>
 <a href="https://openreview.net/profile?id=~guangxiang_zhao2&mode=view">OpenReview</a> <br>
  <p>Email: zhaoguangxiang at pku.edu.cn / guangxiangzhao at gmail.com</p>
</td>
 
</tr>
</table>
 
<p style="font-size: 12px; height: 5px">&nbsp;</p>
 <h3>Research Interests:</h3>
<p>Machine learning methods for natural language processing. </p>

<h3>Academic Activities: </h3>
<ul>
<li>
Conference Member: ACL-2023, NeurIPS-2022, ICML-2022, ICLR-2022, NAACL-2022, ACL-2022, ACL-2021 <br>
<li>
Journal Reviewer: TNNLS <br>
<li>
Secondary Reviewer: EMNLP-2020, ACL-2019 <br>
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>

	<h3>Awards:</h3>
<ul>
<li>
Excellent Graduate of Peking University, 2022 (Top 13%) <br>
<li>
Merit Student of Peking University, 2019 <br>
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>

	
<h3>Papers: </h3>
<ul>
	
<li>
Well-classified Examples are Underestimated in Classification with Deep Neural Networks <br>
<b> Guangxiang Zhao</b>, Wenkai Yang, Xuancheng Ren, Lei Li, Yunfang Wu, Xu Sun.  <br>
AAAI 2022 <br>
TL;DR: In this paper, we find that the cross-entropy loss hinders representation learning, energy optimization, and margin growth, while well-classified examples play a vital role to solving these issues.  We support this finding by both theoretical analysis and empirical results. <br>
<a href=https://arxiv.org/abs/2110.06537>[pdf]</a> <a href=https://github.com/lancopku/well-classified-examples-are-underestimated>[code] </a> <a href=https://github.com/lancopku/well-classified-examples-are-underestimated/blob/main/well_classified_poster.pdf>[poster] </a>
<br><br>

	<li>
Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models <br>
Lei Li, Yankai Lin, Xuancheng Ren, <b> Guangxiang Zhao</b>, Peng Li, Jie Zhou, Xu Sun. <br>
Findings of EMNLP 2022 <br>
TL;DR: In this paper, we explore a novel model reuse paradigm, Knowledge Amalgamation, to merge the knowledge from different teacher-PLMs. <br>
<a href=https://arxiv.org/pdf/2112.07327>[pdf]</a>
<br><br>
	
				<li>
Topology-Imbalance Learning for Semi-Supervised Node Classification <br>
Deli Chen, Yankai Lin, <b> Guangxiang Zhao</b>, Xuancheng Ren, Peng Li, Jie Zhou, Xu Sun.  <br>
NeurIPS 2021 <br>
TL;DR: We identify the problem of Topology-Imbalance and propose the ReNode method as the initial solution. <br>
<a href=https://arxiv.org/abs/2110.04099>[pdf]</a> <a href=https://github.com/victorchen96/ReNode>[code] </a>
<br><br>
		
			<li>
Learning Relation Alignment for Calibrated Cross-modal Retrieval <br>
Shuhuai Ren, Junyang Lin, <b> Guangxiang Zhao</b>, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang. <br>
ACL 2021 <br>
TL;DR: We propose the idea of relation alignment that aligns self-attention among two modalities. <br>
 <a https://aclanthology.org/2021.acl-long.43.pdf>[pdf]</a> <a href=https://github.com/lancopku/IAIS>[code] </a>
<br><br>
				
	<li>
Layer-Wise Multi-View Decoding for Improved Natural Language Generation <br>
Fenglin Liu*, Xuancheng Ren* (Equal Contribution), <b>Guangxiang Zhao</b>, Xu Sun <br>
Preprint 2020 <br>
TL;DR: We find a limitation about information flow in Transformer and propose an effective cross-view decoding method to solve it. <br>
<a href=https://www.researchgate.net/profile/Fenglin-Liu-2/publication/357555021_Layer-Wise_Multi-View_Decoding_for_Improved_Natural_Language_Generation/links/61d3f2bbd450060816894df7/Layer-Wise-Multi-View-Decoding-for-Improved-Natural-Language-Generation.pdf>[pdf]</a>
<br><br>		

						<li>
Understanding and Improving Layer Normalization <br>
Jingjing Xu, Xu Sun, Zhiyuan Zhang,<b> Guangxiang Zhao</b>, Junyang Lin. <br>
NeurIPS 2019 <br>
TL;DR:We find that the back-prop of LayerNorm is essential. We also find that the bias and the gain in LayerNorm increase the risk of over-fitting and do
not work in most cases. <br>
<a href=https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization>[pdf]</a> <a href=https://github.com/lancopku/AdaNorm>[code] </a>
<br><br>
			
	<li>
Parallel Intersected Multi-scale Attention for Sequence to Sequence Learning <br>
<b>Guangxiang Zhao</b>, Xu Sun, Jingjing Xu,  Zhiyuan Zhang, Liangcheng Luo. <br>
Preprint 2019 <br>
TL;DR: We propose a simple module Prime that consistently outperforms the complicated Transformer model on main NMT datasets with SOTA performance by simply stacking this module; We also find that when combine the convolution and self-attention, their operations for learning interactions between tokens should be performed on the same features. <br>

<a href=https://arxiv.org/abs/1911.09483>[pdf]</a> <a href=https://github.com/lancopku/Prime>[code, scripts, and pretrained models] </a> <a href=https://github.com/lancopku/Prime/blob/master/prime_jounal_ver_.pdf>[unified transformer] </a>
<br><br>
		
			<li>
Explicit Sparse Transformer <br>
<b>Guangxiang Zhao</b>, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun. <br>
Revising in Neurocomputing <br>
TL;DR: We propose a sparse attention method without local dependency constraint or the need of predefined sparse attention patterns; We demonstrate that sparse attention (8 or 1/4 of the sequence length(30) in NMT) is better than regular attention. Our method enables extremely sparse attention. E.g., We further improve the sparsity of the state-of-the-art sparse attention of Adaptive Attention Span by $40\times$.  <br>

<a href=https://arxiv.org/pdf/1912.11637>[pdf]</a> <a href=https://github.com/lancopku/Explicit-Sparse-Transformer>[code] </a>  <a href=https://github.com/lancopku/Explicit-Sparse-Transformer/blob/master/Top_k_Selective_Attention__NeuroComputing_.pdf>[extremely sparse transformer] </a> 
<br><br>

	
<li>
Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations <br>
<b>Guangxiang Zhao*</b>, Jingjing Xu* (Equal Contribution), Qi Zeng, Xuancheng Ren, Xu Sun.<br>
NAACL 2019 <br>
TL;DR: We build a multi-label text classification dataset (music styles are hidden in the text) with strong label correlations, propose a method that automatically learns and exploits labels correlation during training. <br>
	
<a href=https://www.aclweb.org/anthology/N19-1296.pdf>[pdf]</a> <a href=https://github.com/lancopku/RMSC>[data] </a>
	
<br><br>

	
		
<div id="ACL_2018d" style="display:none">
<blockquote>
</blockquote>
</div>
</ul>
<a href='https://clustrmaps.com/site/1bmtq'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=300&t=n&d=AHufoLnVxW9stBgBsG6pO16W7mIznSJ4Il0_JjQ7Zao&co=2d78ad&ct=ffffff'/></a>
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=AHufoLnVxW9stBgBsG6pO16W7mIznSJ4Il0_JjQ7Zao&cl=ffffff&w=666"></script> -->
<!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=AHufoLnVxW9stBgBsG6pO16W7mIznSJ4Il0_JjQ7Zao&w=666"></script> -->
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=AHufoLnVxW9stBgBsG6pO16W7mIznSJ4Il0_JjQ7Zao&cl=ffffff&w=666"></script> -->
<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=AHufoLnVxW9stBgBsG6pO16W7mIznSJ4Il0_JjQ7Zao&cl=ffffff&w=a"></script> -->
</body>
</html>
